Welcome to join us!

## Workshop Summary

**Date and Location:**  October 7, 2025, Raleigh, North Carolina, United States

User interfaces for Human-LLM interaction in information-intensive tasks such as sensemaking have predominantly been limited to the chatbot style of interaction. The linear nature of chat poses significant limitations on sensemaking activity that often requires nonlinear thinking, incremental formalization, use of diverse information sources, etc. 

Human cognition in sensemaking tasks exploits a psychological phenomenon called embodied cognition, which externalizes thought processes onto interactive visual workspaces and canvases that enable complex iterative structuring of ideas.  These nonlinear structures often represent various forms of visual languages for sensemaking —- **space to think**.  

While LLMs also exploit the notion of external memory in the form of prompt context or chain of thought, these are often limited to sequential text inputs.  Is there a possibility of marrying the analogous concepts of human embodied cognition and LLM context?  **Can the visual languages of embodied cognition serve as a common ground between human cognition and LLM processing?** **Can LLMs have a form of embodied AI cognition that exploits visual languages?**  **As AI agents become increasingly autonomous, how can visual languages support incremental progression and reduce uncertainty in various sensemaking scenarios, from simple coordination to active collaboration to full teaming, while enabling trust between humans and AI?**  

This workshop seeks contributions that push these frontiers and bring together a multi-disciplinary group of researchers in HCI, AI, cognitive science, visual languages, and computer vision. The workshop will foster a community that will discuss and prepare a forward-looking research agenda for **embodied human-LLM interaction with visual languages**.



<!-- 
## KEYNOTES

**Keynote 1:  Opportunities for Understanding Semantics of User Interactions**

**Abstract:**  Most logging approaches record system events at a fairly low level of abstraction. In this talk, I will argue that higher levels of abstraction are possible and desirable. I will highlight opportunities for increasing semantics that software developers have by carefully recording meaningful events. I will then show that we can leverage algorithmic methods to infer user-intents. Finally, I will show opportunities for eliciting key information from insights directly from users. Explicitly asking users about their intentions has benefits for users, as they can later retrace their steps more efficiently, and system developers, as they can learn more about usage patterns of their system and motivations of their users. There are diverse user input modalities that can provide information at different levels of abstraction and invasiveness.  These modalities range from multiple choice responses, to structured notes, to "think-aloud-like" approaches. In combination, these approaches are promising for building systems that have a better understanding of their users and hence can support users in their analytical tasks.

**Biography:**  Alex is an Associate Professor of Computer Science at the Scientific Computing and Imaging Institute and the School of Computing at the University of Utah. I direct the Visualization Design Lab where we develop visualization methods and systems to help solve today's scientific problems.  Before joining the University of Utah, he was a lecturer and post-doctoral visualization researcher at Harvard University. He received his PhD, master's, and undergraduate degrees from Graz University of Technology. In 2011, he was a visiting researcher at Harvard Medical School.  He is the recipient of an NSF CAREER award and multiple best paper awards or best paper honorable mentions at IEEE VIS, ACM CHI, and other conferences. He also received a best dissertation award from his alma mater.  He co-founded Datavisyn (http://datavisyn.io), a startup company developing visual analytics solutions for the pharmaceutical industry, where he is currently spending his sabbatical. 


**Keynote 2:  User Interaction in Visual Analytics: Beyond Ephemeral Events**

**Abstract:**  User interaction in visual analytic tools is often treated as an event that causes a direct response in the system based on the operation performed. While effective for fostering exploration and analysis of people using these systems, user interactions contain signals about the people they were performed by. In this talk, I discuss opportunities for promoting user interactions to first order objects that can be used for a plethora of useful tasks by the system, including detecting cognitive biases that may exist, guiding and steering machine learning models, and more. I'll provide examples of past and current research that explores this direction, and discuss potential future directions.

**Biography:**  Alex Endert is an Associate Professor in the School of Interactive Computing at the Georgia Institute of Technology. He directs the Visual Analytics Lab and conducts research to help people make sense of data and models through interactive visualizations and visual analytic systems. The lab’s research is also often tested in practice in domains such as intelligence analysis, cyber security, manufacturing safety, and others. Our lab’s work is funded by NSF, DARPA, DOD, DHS, NIJ, and generous industry partners. In 2018, He was awarded an NSF CAREER Award for work on Visual Analytics by Demonstration. He received his Ph.D. in Computer Science from Virginia Tech in 2012. In 2013, his work on Semantic Interaction was awarded the IEEE VGTC VPG Pioneers Group Doctoral Dissertation Award, and the Virginia Tech Computer Science Best Dissertation Award. -->

## Topics of Interest
Topics of interest include but are not limited to:
- Design or evaluation of visual languages for AI
- User interface or visualization design for human-AI interaction
- Theories for human-AI common ground in sensemaking
- Methods for human steering or explainability of AI sensemaking
- NLP methods for augmenting LLMs with embodied cognition capabilities
- Models for LLM processing of visual languages
- Evaluations of human-AI teaming for sensemaking


## Call For Participation
Two types of contributions are sought, each with separate deadlines and review criteria.

**Research Papers:**  Manuscripts that contain results suitable for publication, which will be presented at the workshop.  Research paper submissions will be peer-reviewed by a minimum of one organizer and one knowledgeable external reviewer recruited by the organizers to ensure relevance, quality, and soundness of results. A length of 4-8 pages is required. The VL/HCC Conference plans to publish accepted workshop manuscripts in an accompanying volume published by IEEE.

Submission deadline:  Friday, July 11

Acceptance notification:  Friday, August 1

Camera-ready deadline following minor revisions:  Friday, August 8

Format: Current IEEE two-column conference paper format - "US letter" [template](https://www.ieee.org/conferences/publishing/templates)

Submissions should be uploaded to [EasyChair](https://easychair.org/my/conference?conf=vlhcc2025):  
Register or Log in -> author -> New Submission -> Select our workshop "Beyond Chat"

**Position Statements:**  Statements relating to a topic of interest that can be discussed at the workshop.  Position statements will be reviewed by the organizers to ensure relevance to the workshop.  A length of 1-4 pages is required. 

Submission deadline:  Friday, August 22

Acceptance notification:  Friday, August 29

Submission method: Submissions should be uploaded to [EasyChair](https://easychair.org/my/conference?conf=vlhcc2025):  
Register or Log in -> author -> New Submission -> Select our workshop "Beyond Chat" (submitted as paper)

<!-- Submissions by email? -->



<!-- ## WORKSHOP TOPICS

The topic of the workshop will focus on issues and opportunities related to the use of machine learning to learn from user interaction in the course of data visualization and analysis. Specifically, we will focus on research questions including:

- How are machine learning algorithms currently learning from user interaction, and what other possibilities exist?
- What kinds of interactions can provide feedback to machine learning algorithms?
- What can machine learning algorithms learn from interactions?
- Which machine learning algorithms are most applicable in this domain?
- How can machine learning algorithms be designed to enable user interaction and feedback?
- How can visualizations and interactions be designed to exploit machine learning algorithms?
- How can visualization system architectures be designed to support machine learning?
- How should we manage conflicts between the user's intent and the data or machine learning algorithm capabilities?
- How can we evaluate systems that incorporate both machine learning algorithms and user interaction together?
- How can machine learning and user interaction together make both computation and user cognition more efficient?
- How can we support the sensemaking process by learning from user interaction? -->


<!-- ## SUBMISSIONS

This year, we plan to accept both short and long papers jointly in the same submission block.  Full papers have a length of 5-10 pages (not including references), while short papers are 2-4 pages (plus references).  Short papers are intended to capture either (1) limited aspects of a larger work that fit our call or (2) late-breaking work not yet mature enough for a full paper submission.  The option of submitting a short paper replaces the posters track that we previously offered at MLUI.  

Submissions should be uploaded to the [MLUI 2021 track on PCS](https://new.precisionconference.com/submissions), which can be found under VGTC->VIS2021.  All submissions will be reviewed by a committee of reviewers that we will organize.  This committee will include the workshop committee members. The size of the committee will be determined by the number of submissions, such that each submission is reviewed by at least 2 committee members.  Both full and short paper metadata (author information, title, university, etc.) as well as the submissions themselves will be posted to the workshop website in advance of the event.  Workshop papers will be archived on IEEE Xplore following the conference.
   -->
  
<!-- #### Important Dates

Submission deadline:  July 30, 2021

Author notification:  August 31, 2021

Camera-ready deadline:  September 10, 2021 -->

## WORKSHOP SCHEDULE

**Session:** 9:00am-12:30pm, October 7, 2025
- (9:00--9:10) **Welcome and Charge**
- (9:10--10:10) **Paper presentations or demos**
- (10:10--10:40) **Lightning talks/position statements**
- (10:40--11:00) **Break**
- (11:00--11:40) **Group brainstorming session**
- (11:40--12:20) **Collaborative affinity diagramming**
- (12:20--12:30) **Closing and action items**


## ORGANIZERS

Chris North, Virginia Tech  (Contact: human-ai-sensemaking@googlegroups.com)

Joel Chan, University of Maryland

Rebecca Faust, Tulane University

Xuan Wang, Virginia Tech

John Wenskovitch, Pacific Northwest National Laboratory


## STUDENT ORGANIZERS

Xuxin Tang, Virginia Tech

Siyi Zhu, University of Maryland